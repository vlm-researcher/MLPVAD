{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'git+https://github.com/salesforce/LAVIS.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_preprocess, txt_preprocess = load_model_and_preprocess(\"blip_diffusion\", \"base\", device=\"cuda\", is_eval=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This demo shows how to render different renditions of the given subject in a zero-shot setup.\n",
    "\n",
    "(1) extracting BLIP-2 embeddings on ``cond_subject`` and ``cond_image``.\n",
    "\n",
    "(2) Generating on prompts: \"A ``${BLIP-2 embedding} ${tgt_subject} ${text_prompt}``\".\n",
    "\n",
    "### Tips\n",
    "``tgt_subject`` can be a different subject from the ``cond_subject``. For example, if ``cond_subject=\"dog\"`` (and you use a dog image as condition), and ``tgt_subject==\"tiger\"``, you'd expect the model to generate a tiger that looks like this particular dog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_subject = \"dog\"\n",
    "tgt_subject = \"dog\"\n",
    "# prompt = \"painting by van gogh\"\n",
    "text_prompt = \"swimming underwater\"\n",
    "\n",
    "cond_subjects = [txt_preprocess[\"eval\"](cond_subject)]\n",
    "tgt_subjects = [txt_preprocess[\"eval\"](tgt_subject)]\n",
    "text_prompt = [txt_preprocess[\"eval\"](text_prompt)]\n",
    "\n",
    "cond_image = Image.open(\"../images/dog.png\").convert(\"RGB\")\n",
    "display(cond_image.resize((256, 256)))\n",
    "\n",
    "cond_images = vis_preprocess[\"eval\"](cond_image).unsqueeze(0).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"cond_images\": cond_images,\n",
    "    \"cond_subject\": cond_subjects,\n",
    "    \"tgt_subject\": tgt_subjects,\n",
    "    \"prompt\": text_prompt,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output = 4\n",
    "\n",
    "iter_seed = 88888\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n",
    "\n",
    "for i in range(num_output):\n",
    "    output = model.generate(\n",
    "        samples,\n",
    "        seed=iter_seed + i,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        neg_prompt=negative_prompt,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "\n",
    "    display(output[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
