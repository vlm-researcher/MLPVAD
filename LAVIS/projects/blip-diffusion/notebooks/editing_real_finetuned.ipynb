{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'git+https://github.com/salesforce/LAVIS.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_preprocess, txt_preprocess = load_model_and_preprocess(\"blip_diffusion\", \"base\", device=\"cuda\", is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This demo shows how to edit a **real** image with a finetuned checkpoint on a given subject. It works in the following steps:\n",
    "\n",
    "(1) load the finetuned checkpoint.\n",
    "\n",
    "(2) run DDIM inversion on the given image using prompt ``A ${src_subject} ${prompt}.``;\n",
    "\n",
    "(3) extracting BLIP-2 embeddings on condition subject image, using ``cond_subject`` and ``cond_image``.\n",
    "\n",
    "(4) edit the real image with the subject visuals, using the prompt ``A ${BLIP-2 embedding} ${tgt_subject} ${prompt}`` and the DDIM inverted latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_subject = \"dog\"\n",
    "src_subject = \"cat\"\n",
    "tgt_subject = \"dog\"\n",
    "\n",
    "text_prompt = \"sit on sofa\"\n",
    "\n",
    "cond_subject = txt_preprocess[\"eval\"](cond_subject)\n",
    "src_subject = txt_preprocess[\"eval\"](src_subject)\n",
    "tgt_subject = txt_preprocess[\"eval\"](tgt_subject)\n",
    "text_prompt = [txt_preprocess[\"eval\"](text_prompt)]\n",
    "\n",
    "src_image = Image.open(\"../images/cat-sofa.png\").convert(\"RGB\")\n",
    "display(src_image.resize((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_ckpt = \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP-Diffusion/db-dog/checkpoint_40.pth\"\n",
    "# can also use a local checkpoint\n",
    "# finetuned_ckpt = \"../checkpoints/db-dog/checkpoint_40.pth\"\n",
    "model.load_checkpoint(finetuned_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"cond_images\": None,\n",
    "    \"cond_subject\": cond_subject,\n",
    "    \"src_subject\": src_subject,\n",
    "    \"tgt_subject\": tgt_subject,\n",
    "    \"prompt\": text_prompt,\n",
    "    \"raw_image\": src_image,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_seed = 8887\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50 \n",
    "num_inversion_steps = 50 # increase to improve DDIM inversion quality\n",
    "lb_threshold = 0.3 # increase to edit fewer pixels.\n",
    "negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n",
    "\n",
    "output = model.edit(\n",
    "    samples,\n",
    "    seed=iter_seed,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    num_inversion_steps=num_inversion_steps,\n",
    "    neg_prompt=negative_prompt,\n",
    "    lb_threshold=lb_threshold,\n",
    ")\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(\"Before editing:\")\n",
    "display(output[0])\n",
    "\n",
    "print(\"After editing:\")\n",
    "display(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
