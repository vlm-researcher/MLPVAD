{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from lavis.models.blip_diffusion_models.utils import preprocess_canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_preprocess, txt_preprocess = load_model_and_preprocess(\n",
    "    \"blip_diffusion\", \"canny\", device=\"cuda\", is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_canny(cond_image_input, low_threshold, high_threshold):\n",
    "    # convert cond_image_input to numpy array\n",
    "    cond_image_input = np.array(cond_image_input).astype(np.uint8)\n",
    "\n",
    "    # canny_input, vis_control_image = preprocess_canny(cond_image_input, 512, low_threshold=100, high_threshold=200)\n",
    "    vis_control_image = preprocess_canny(cond_image_input, 512, low_threshold=low_threshold, high_threshold=high_threshold)\n",
    "\n",
    "    return vis_control_image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This demo shows how to transfer visuals of a style image for stylization. It works in the following steps:\n",
    "\n",
    "1. The model extract BLIP-embeddings from ``style_subject`` and ``style_image``.\n",
    "2. Extract canny edges from ``cldm_cond_image``.\n",
    "3. The model performs stylization using the prompt \"A ``${BLIP-embedding} ${tgt_subject} ${text_prompt}``\", with ``cldm_cond_image`` as structural control.\n",
    "\n",
    "Tips:\n",
    "1. It is suggested use prompt that is aligned with the edge map. Otherwise, the model won't produce generations respecting the prompts.\n",
    "\n",
    "2. Using condition images with clean background helps to better encode the style, especially the subject in the condition image is highly personalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_subject = \"flower\" # subject that defines the style\n",
    "tgt_subject = \"teapot\"  # subject to generate.\n",
    "\n",
    "text_prompt = \"on a marble table\"\n",
    "\n",
    "cond_subjects = [txt_preprocess[\"eval\"](style_subject)]\n",
    "tgt_subjects = [txt_preprocess[\"eval\"](tgt_subject)]\n",
    "text_prompt = [txt_preprocess[\"eval\"](text_prompt)]\n",
    "\n",
    "cldm_cond_image = Image.open(\"../images/kettle.jpg\").convert(\"RGB\")\n",
    "\n",
    "style_image = Image.open(\"../images/flower.jpg\").convert(\"RGB\")\n",
    "display(style_image.resize((256, 256), resample=Image.BILINEAR))\n",
    "style_image = vis_preprocess[\"eval\"](style_image).unsqueeze(0).cuda()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detection\n",
    "\n",
    "The quality of canny edge detection is important to create visually pleasant generations.\n",
    "\n",
    "We generally find using dense canny edge maps can better control the generation.\n",
    "In contrast, if edges are sparse, the model will find it hard to clearly segment target subject from background, causing styling effect to extend beyond subject silhouette or blurry generation.\n",
    "\n",
    "Two important factors to create dense canny maps:\n",
    "1. to select texture-rich subjects in preference to textureless subjects. Particular art styles are usually not texture-rich, such as anime, cartoon etc, and may be less ideal.\n",
    "2. to experiment with different canny thresholding. Any edges with intensity gradient more than canny_high_threshold are sure to be edges. Those below canny_low_threshold are sure to be non-edges, and will be discarded. See section 5: https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html for reference.\n",
    "**Namely, if dense edges are preferred, setting low values for both threshold helps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "canny_low_threshold = 30\n",
    "canny_high_threshold = 70\n",
    "\n",
    "cond_image_input = generate_canny(cldm_cond_image, canny_low_threshold, canny_high_threshold)\n",
    "\n",
    "cond_image_display = cond_image_input.resize((256, 256), resample=Image.BILINEAR)\n",
    "display(cond_image_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"cond_images\": style_image,\n",
    "    \"cond_subject\": cond_subjects,\n",
    "    \"tgt_subject\": tgt_subjects,\n",
    "    \"prompt\": text_prompt,\n",
    "    \"cldm_cond_image\": cond_image_input.convert(\"RGB\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output = 2\n",
    "\n",
    "iter_seed = 88888\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n",
    "\n",
    "\n",
    "for i in range(num_output):\n",
    "    output = model.generate(\n",
    "        samples,\n",
    "        seed=iter_seed + i,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        neg_prompt=negative_prompt,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "\n",
    "    display(output[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
