{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032a8c68",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2prompt-vqa/img2prompt_vqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa48dc",
   "metadata": {
    "id": "_qxQ4_bEhkrd"
   },
   "source": [
    "## Img2Prompt-VQA: Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd92a8",
   "metadata": {
    "id": "dq8t0LJThhuJ"
   },
   "outputs": [],
   "source": [
    "# install requirements\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Colab.')\n",
    "    !git clone https://github.com/salesforce/LAVIS\n",
    "    %cd LAVIS\n",
    "    !pip install .\n",
    "    !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "else:\n",
    "    !pip install omegaconf\n",
    "    %cd ../..\n",
    "    !pip install .\n",
    "    !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "\n",
    "%cd projects/img2prompt-vqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11ce1e",
   "metadata": {
    "id": "838168ff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lavis.common.gradcam import getAttMap\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a0e61",
   "metadata": {},
   "source": [
    "### Load LLM to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def load_model(model_selection):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_selection)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_selection, use_fast=False)\n",
    "    return model,tokenizer\n",
    "\n",
    "# Choose LLM to use\n",
    "# weights for OPT-6.7B/OPT-13B/OPT-30B/OPT-66B will download automatically\n",
    "print(\"Loading Large Language Model (LLM)...\")\n",
    "llm_model, tokenizer = load_model('facebook/opt-6.7b')  # ~13G (FP16)\n",
    "# llm_model, tokenizer = load_model('facebook/opt-13b') # ~26G (FP16)\n",
    "# llm_model, tokenizer = load_model('facebook/opt-30b') # ~60G (FP16)\n",
    "# llm_model, tokenizer = load_model('facebook/opt-66b') # ~132G (FP16)\n",
    "\n",
    "# you need to manually download weights, in order to use OPT-175B\n",
    "# https://github.com/facebookresearch/metaseq/tree/main/projects/OPT\n",
    "# llm_model, tokenizer = load_model('facebook/opt-175b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63221f",
   "metadata": {
    "id": "2ffeec4e"
   },
   "source": [
    "### Load an example image and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21675a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "2da65ed1",
    "outputId": "31b7806a-95eb-418f-fed9-e55ee9cc51fd"
   },
   "outputs": [],
   "source": [
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/projects/pnp-vqa/demo.png'\n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "raw_image = Image.open(\"./demo.png\").convert(\"RGB\")\n",
    "question = \"What item s are spinning which can be used to control electric?\"\n",
    "print(question)\n",
    "display(raw_image.resize((400, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078b2c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ba89c01",
    "outputId": "04e3ab39-475b-4c8c-b335-45e0b739cc6e"
   },
   "outputs": [],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780851fd",
   "metadata": {
    "id": "076f55e8"
   },
   "source": [
    "### Load Img2Prompt-VQA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693e281",
   "metadata": {
    "id": "01be941e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"img2prompt_vqa\", model_type=\"base\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62525c37",
   "metadata": {
    "id": "f0e27d11"
   },
   "source": [
    "### Preprocess image and text inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bac633",
   "metadata": {
    "id": "1c47f415"
   },
   "outputs": [],
   "source": [
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "question = txt_processors[\"eval\"](question)\n",
    "\n",
    "samples = {\"image\": image, \"text_input\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b9c7a",
   "metadata": {
    "id": "ba7a0e77"
   },
   "source": [
    "### Img2Prompt-VQA utilizes 4 submodels to perform VQA:\n",
    "#### 1. Image-Question Matching \n",
    "Compute the relevancy score of image patches with respect to the question using GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb03d8d",
   "metadata": {
    "id": "6e1615fc"
   },
   "outputs": [],
   "source": [
    "samples = model.forward_itm(samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda0e84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "46c50f9c",
    "outputId": "1588f183-c786-4de0-abc8-ecbfffe2f9a6"
   },
   "outputs": [],
   "source": [
    "# Gradcam visualisation\n",
    "dst_w = 720\n",
    "w, h = raw_image.size\n",
    "scaling_factor = dst_w / w\n",
    "\n",
    "resized_img = raw_image.resize((int(w * scaling_factor), int(h * scaling_factor)))\n",
    "norm_img = np.float32(resized_img) / 255\n",
    "gradcam = samples['gradcams'].reshape(24,24)\n",
    "\n",
    "avg_gradcam = getAttMap(norm_img, gradcam, blur=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.imshow(avg_gradcam)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "print('Question: {}'.format(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2b67b",
   "metadata": {
    "id": "c2d7cf41"
   },
   "source": [
    "#### 2. Image Captioning\n",
    "Generate question-guided captions based on the relevancy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc925ece",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49ac307b",
    "outputId": "93535276-fdd0-4620-a833-cc7da0ff4776"
   },
   "outputs": [],
   "source": [
    "samples = model.forward_cap(samples=samples, num_captions=50, num_patches=20)\n",
    "print('Examples of question-guided captions: ')\n",
    "samples['captions'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c40a",
   "metadata": {
    "id": "b708efb6"
   },
   "source": [
    "#### 3. Question Generation\n",
    "Generate synthetic questions using the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b50ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f065b898",
    "outputId": "152ef7d0-9071-4397-f68f-00534a274784"
   },
   "outputs": [],
   "source": [
    "samples = model.forward_qa_generation(samples)\n",
    "print('Sample Question: {} \\nSample Answer: {}'.format(samples['questions'][:5], samples['answers'][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['questions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb868a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. Prompt Construction\n",
    "Prepare the prompts for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4db44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Img2Prompt = model.prompts_construction(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df71c75",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. Load LLM and Predict Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dfecf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In this notebook, we only use CPU for LLM inference\n",
    "# To run inference on GPU, see https://github.com/CR-Gjx/Img2Prompt for reference\n",
    "device = \"cpu\"\n",
    "\n",
    "def postprocess_Answer(text):\n",
    "    for i, ans in enumerate(text):\n",
    "        for j, w in enumerate(ans):\n",
    "            if w == '.' or w == '\\n':\n",
    "                ans = ans[:j].lower()\n",
    "                break\n",
    "    return ans\n",
    "\n",
    "Img2Prompt_input = tokenizer(Img2Prompt, padding='longest', truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "assert (len(Img2Prompt_input.input_ids[0])+20) <=2048\n",
    "\n",
    "outputs_list  = []\n",
    "outputs = llm_model.generate(input_ids=Img2Prompt_input.input_ids,\n",
    "                         attention_mask=Img2Prompt_input.attention_mask,\n",
    "                         max_length=20+len(Img2Prompt_input.input_ids[0]),\n",
    "                         return_dict_in_generate=True,\n",
    "                         output_scores=True\n",
    "                         )\n",
    "outputs_list.append(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470640e",
   "metadata": {},
   "source": [
    "#### 5. Decoding to answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7363d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_list\n",
    "\n",
    "pred_answer = tokenizer.batch_decode(outputs.sequences[:, len(Img2Prompt_input.input_ids[0]):])\n",
    "pred_answer = postprocess_Answer(pred_answer)\n",
    "\n",
    "print({\"question\": question, \"answer\": pred_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd1a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
